name: ggml llama2 examples

on:
  schedule:
    - cron: "0 0 * * *"
  workflow_dispatch:
    inputs:
      logLevel:
        description: 'Log level'
        required: true
        default: 'info'
  push:
    branches: [ '*' ]
    paths:
      - ".github/workflows/llama.yml"
      - "wasmedge-ggml-llama/**"
  pull_request:
    branches: [ '*' ]
    paths:
      - ".github/workflows/llama.yml"
      - "wasmedge-ggml-llama/**"

jobs:
  build:

    runs-on: ubuntu-20.04

    steps:
    - uses: actions/checkout@v3

    - name: Install apt-get packages
      run: |
        sudo ACCEPT_EULA=Y apt-get update
        sudo ACCEPT_EULA=Y apt-get upgrade
        sudo apt-get install wget git curl software-properties-common build-essential

    - name: Install Rust target for wasm
      run: |
        rustup target add wasm32-wasi

    - name: Install WasmEdge + WASI-NN + GGML
      run: |
        VERSION=0.13.4
        curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | sudo bash -s -- -v $VERSION --plugins wasi_nn-ggml -p /usr/local

    - name: Example
      run: |
        cd wasmedge-ggml-llama
        # curl -LO https://huggingface.co/kirp/TinyLlama-1.1B-Chat-v0.2-gguf/resolve/main/ggml-model-q4_0.gguf
        curl -LO https://huggingface.co/juanjgit/orca_mini_3B-GGUF/resolve/main/orca-mini-3b.q4_0.gguf
        cargo build --target wasm32-wasi --release
        wasmedge compile target/wasm32-wasi/release/wasmedge-ggml-llama.wasm wasmedge-ggml-llama.wasm
        # wasmedge --dir .:. --nn-preload default:GGML:CPU:ggml-model-q4_0.gguf wasmedge-ggml-llama.wasm default '<|im_start|>user\nWhere is the capital of Japan?<|im_end|>\n<|im_start|>assistant\n'
        wasmedge --dir .:. --nn-preload default:GGML:CPU:orca-mini-3b.q4_0.gguf wasmedge-ggml-llama.wasm default '### System:\nYou are an AI assistant\n\n### User:\nWhere is the capital of Japan?\n\n### Response:\n'
