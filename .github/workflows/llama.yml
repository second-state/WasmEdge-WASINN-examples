name: ggml llama2 examples

on:
  schedule:
    - cron: "0 0 * * *"
  workflow_dispatch:
    inputs:
      logLevel:
        description: 'Log level'
        required: true
        default: 'info'
  push:
    branches: [ '*' ]
    paths:
      - ".github/workflows/llama.yml"
      - "wasmedge-ggml-llama/**"
  pull_request:
    branches: [ '*' ]
    paths:
      - ".github/workflows/llama.yml"
      - "wasmedge-ggml-llama/**"

jobs:
  build:

    runs-on: ubuntu-20.04

    steps:
    - uses: actions/checkout@v3

    - name: Install apt-get packages
      run: |
        echo RESET grub-efi/install_devices | sudo debconf-communicate grub-pc
        sudo ACCEPT_EULA=Y apt-get update
        sudo ACCEPT_EULA=Y apt-get upgrade
        sudo apt-get install wget git curl software-properties-common build-essential libopenblas-dev

    - name: Install Rust target for wasm
      run: |
        rustup target add wasm32-wasi

    - name: Install WasmEdge + WASI-NN + GGML
      run: |
        VERSION=0.13.4
        curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | sudo bash -s -- -v $VERSION --plugins wasi_nn-ggml -p /usr/local

    - name: Example
      run: |
        cd wasmedge-ggml-llama-interactive
        curl -LO https://huggingface.co/juanjgit/orca_mini_3B-GGUF/resolve/main/orca-mini-3b.q4_0.gguf
        cargo build --target wasm32-wasi --release
        wasmedge compile target/wasm32-wasi/release/wasmedge-ggml-llama.wasm wasmedge-ggml-llama.wasm
        wasmedge --dir .:. \
          --env enable_log=false \
          --env stream_stdout=false \
          --nn-preload default:GGML:CPU:orca-mini-3b.q4_0.gguf \
          wasmedge-ggml-llama-interactive.wasm default \
          '### System:\nYou are an AI assistant\n\n### User:\nWhere is the capital of Japan?\n\n### Response:\n'
